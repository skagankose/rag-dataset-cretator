services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8051:8051"
    environment:
      - APP_ENV=dev
      - BACKEND_PORT=8051
      - DATA_DIR=/app/data
      # LLM Provider Configuration
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      # OpenAI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_CHAT_MODEL=${OPENAI_CHAT_MODEL:-gpt-4o}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-60}
      - OPENAI_MAX_RETRIES=${OPENAI_MAX_RETRIES:-5}
      # Gemini Configuration
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_CHAT_MODEL=${GEMINI_CHAT_MODEL:-gemini-1.5-flash}
      - GEMINI_TIMEOUT=${GEMINI_TIMEOUT:-60}
      - GEMINI_MAX_RETRIES=${GEMINI_MAX_RETRIES:-5}
      # Ollama Configuration
      - OLLAMA_API_BASE=${OLLAMA_API_BASE:-http://host.docker.internal:11434}
      - OLLAMA_CHAT_MODEL=${OLLAMA_CHAT_MODEL:-llama3.2}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-120}
      - OLLAMA_MAX_RETRIES=${OLLAMA_MAX_RETRIES:-3}
      # Ingestion Configuration
      - DEFAULT_CHUNK_SIZE=${DEFAULT_CHUNK_SIZE:-1200}
      - DEFAULT_CHUNK_OVERLAP=${DEFAULT_CHUNK_OVERLAP:-200}
      - DEFAULT_TOTAL_QUESTIONS=${DEFAULT_TOTAL_QUESTIONS:-10}
      - STRIP_SECTIONS=${STRIP_SECTIONS:-true}
      # Prompts Configuration
      - PROMPT_LANGUAGE=${PROMPT_LANGUAGE:-en}
    volumes:
      - ./backend:/app
      - ./backend/data:/app/data
    command: uvicorn app.main:app --host 0.0.0.0 --port 8051 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8051/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "5180:5180"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8051
    command: npm run dev -- --host

 